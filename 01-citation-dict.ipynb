{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e121d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 04:38:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/08 04:38:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import json, pickle as pkl\n",
    "from unidecode import unidecode\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product, combinations\n",
    "from copy import deepcopy\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm, trange\n",
    "tqdm.pandas(ncols=100, mininterval=1)\n",
    "tqdm, trange = partial(tqdm, ncols=100, mininterval=1), partial(trange, ncols=100, mininterval=1)\n",
    "\n",
    "conf_list = [\n",
    "    ('spark.app.name', 'Shaoerzhuo@TeamScience'),\n",
    "    ('spark.local.dir', '/tmp/spark'),\n",
    "    ('spark.rdd.compress', 'true'),\n",
    "    ('spark.driver.memory', '1024g'),\n",
    "    ('spark.driver.maxResultSize', '200g'),\n",
    "    ('spark.serializer.objectStreamReset', '100'),\n",
    "    ('spark.master', 'local[48]'),\n",
    "    ('spark.submit.deployMode', 'client'),\n",
    "    ('spark.ui.showConsoleProgress', 'false'),\n",
    "\n",
    "    ('spark.driver.extraJavaOptions', '-Djava.security.manager=allow'), \n",
    "    ('spark.executor.extraJavaOptions', '-Djava.security.manager=allow'),\n",
    "\n",
    "    ('spark.sql.adaptive.enabled', 'true'),\n",
    "    ('spark.sql.adaptive.coalescePartitions.enabled', 'true'),\n",
    "    ('spark.sql.adaptive.localShuffleReader.enabled', 'true'),\n",
    "    ('spark.sql.adaptive.skewJoin.enabled', 'true'),\n",
    "    ('spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes', str(256 * 1024 * 1024)),  # 256MB\n",
    "]\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, explode_outer, udf, size, lower\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "if 'sc' not in locals():\n",
    "    conf = SparkConf().setAll(conf_list)\n",
    "    sc = SparkContext(conf = conf)\n",
    "    sc.setLogLevel('ERROR')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6126708",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def paper_id_to_int(paper_id):\n",
    "    return int(paper_id[4:])\n",
    "\n",
    "citation_by_time = spark.read.parquet(\"../parquet/processed/pub_citing_cited_years.parquet\")\n",
    "\n",
    "citation_by_time = citation_by_time.select(\n",
    "    paper_id_to_int(\"citing_paperid\").cast(T.LongType()).alias(\"citing_paperid\"),\n",
    "    paper_id_to_int(\"cited_paperid\").cast(T.LongType()).alias(\"cited_paperid\"),\n",
    "    F.col(\"citing_year\").cast(T.LongType()).alias(\"citing_year\"),\n",
    "    F.col(\"cited_year\").cast(T.LongType()).alias(\"cited_year\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67376bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict_cited_to_citing using Spark DataFrame operations\n",
    "# Test with only top 10000 rows\n",
    "citation_by_time_sample = citation_by_time\n",
    "\n",
    "dict_cited_to_citing = citation_by_time_sample.groupBy(\"cited_paperid\").agg(\n",
    "    F.collect_list(F.struct(\"citing_paperid\", \"citing_year\")).alias(\"citing_list\")\n",
    ")\n",
    "\n",
    "dict_citing_to_cited = citation_by_time_sample.groupBy(\"citing_paperid\").agg(\n",
    "    F.collect_list(F.struct(\"cited_paperid\", \"cited_year\")).alias(\"cited_list\")\n",
    ")\n",
    "\n",
    "dict_paper_id_to_year = citation_by_time_sample.select(\n",
    "    col(\"cited_paperid\").alias(\"paperid\"),\n",
    "    col(\"cited_year\").alias(\"year\")\n",
    ").union(\n",
    "    citation_by_time_sample.select(\n",
    "        col(\"citing_paperid\").alias(\"paperid\"),\n",
    "        col(\"citing_year\").alias(\"year\")\n",
    "    )\n",
    ").distinct()\n",
    "\n",
    "# Save to parquet files instead of converting to dict\n",
    "dict_cited_to_citing.write.mode(\"overwrite\").parquet(\"intermediate/dict_cited_to_citing_id_and_year.parquet\")\n",
    "dict_citing_to_cited.write.mode(\"overwrite\").parquet(\"intermediate/dict_citing_to_cited_id_and_year.parquet\")\n",
    "dict_paper_id_to_year.write.mode(\"overwrite\").parquet(\"intermediate/dict_paper_id_to_year.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7002d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = glob('intermediate/dict_paper_id_to_year.parquet/*.parquet')\n",
    "%time dict_paper_id_to_year = pd.concat([pd.read_parquet(f) for f in tqdm(l)])\n",
    "%time dict_paper_id_to_year.to_feather('data/dict_paper_id_to_year.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb084923",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = glob('intermediate/dict_citing_to_cited_id_and_year.parquet/*.parquet')\n",
    "%time dict_citing_to_cited_id_and_year = pd.concat([pd.read_parquet(f) for f in tqdm(l)])\n",
    "dict_citing_to_cited_id_and_year[\"cited_list\"] = dict_citing_to_cited_id_and_year[\"cited_list\"].progress_map(\n",
    "    lambda x: [(i[\"cited_paperid\"], i[\"cited_year\"]) for i in x])\n",
    "%time dict_citing_to_cited_id_and_year.to_feather('data/dict_citing_to_cited_id_and_year.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefeeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = glob('intermediate/dict_cited_to_citing_id_and_year.parquet/*.parquet')\n",
    "%time dict_cited_to_citing_id_and_year = pd.concat([pd.read_parquet(f) for f in tqdm(l)])\n",
    "dict_cited_to_citing_id_and_year[\"citing_list\"] = dict_cited_to_citing_id_and_year[\"citing_list\"].progress_map(\n",
    "    lambda x: [(i[\"citing_paperid\"], i[\"citing_year\"]) for i in x])\n",
    "%time dict_cited_to_citing_id_and_year.to_feather('data/dict_cited_to_citing_id_and_year.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bb5951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 56s, sys: 1min 19s, total: 8min 15s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "papers = pd.read_parquet('../parquet/processed/publications.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f83bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 10.6 s, total: 37.2 s\n",
      "Wall time: 37 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 421 ms, sys: 295 ms, total: 716 ms\n",
      "Wall time: 716 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 152327403/152327403 [02:06<00:00, 1199711.78it/s]\n",
      "<timed exec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 5.22 s, total: 2min 7s\n",
      "Wall time: 2min 7s\n",
      "CPU times: user 13.5 s, sys: 2.38 s, total: 15.9 s\n",
      "Wall time: 15.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1665</td>\n",
       "      <td>[1051816633, 1001261415, 1008254293, 102786731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1666</td>\n",
       "      <td>[1046462870, 1034167481, 1011608840, 103042842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1667</td>\n",
       "      <td>[1086930131, 1087186495, 1099352398, 109950846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1668</td>\n",
       "      <td>[1040983623, 1038172664, 1035491401, 108718649...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1669</td>\n",
       "      <td>[1044301911, 1016722741, 1086858230, 102238151...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                           paper_id\n",
       "0  1665  [1051816633, 1001261415, 1008254293, 102786731...\n",
       "1  1666  [1046462870, 1034167481, 1011608840, 103042842...\n",
       "2  1667  [1086930131, 1087186495, 1099352398, 109950846...\n",
       "3  1668  [1040983623, 1038172664, 1035491401, 108718649...\n",
       "4  1669  [1044301911, 1016722741, 1086858230, 102238151..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time papers_2 = papers.dropna(subset=[\"year\", \"id\"])\n",
    "%time papers_2[\"year\"] = papers_2[\"year\"].astype(int)\n",
    "%time papers_2[\"paper_id\"] = papers_2[\"id\"].progress_map(lambda x: int(x[4:]))\n",
    "%time dict_year_to_paper_ids = papers_2.groupby(\"year\")[\"paper_id\"].apply(list).reset_index()\n",
    "\n",
    "dict_year_to_paper_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b75869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 570 ms, total: 12.4 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "dict_year_to_paper_ids.sort_values(\"year\", inplace=True)\n",
    "%time dict_year_to_paper_ids.to_feather('intermediate/dict_year_to_paper_ids.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08900d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.7 s, sys: 12 s, total: 56.7 s\n",
      "Wall time: 56.4 s\n",
      "CPU times: user 28.8 s, sys: 4 s, total: 32.8 s\n",
      "Wall time: 32.8 s\n",
      "CPU times: user 28.2 s, sys: 2.68 s, total: 30.9 s\n",
      "Wall time: 30.9 s\n",
      "CPU times: user 583 ms, sys: 211 ms, total: 794 ms\n",
      "Wall time: 793 ms\n",
      "CPU times: user 17.5 s, sys: 1.29 s, total: 18.8 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "papers_2 = papers.dropna(subset=[\"id\", \"journal_id\"]).reset_index(drop=True)\n",
    "papers_2[\"paper_id\"] = papers_2[\"id\"].str.replace(\"pub.\", \"\").astype(int)\n",
    "papers_2[\"journal_id\"] = papers_2[\"journal_id\"].str.replace(\"jour.\", \"\").astype(int)\n",
    "dict_paper_to_journal_id = papers_2[[\"paper_id\", \"journal_id\"]]\n",
    "dict_paper_to_journal_id.columns = [\"paper_id\", \"journal_id\"]\n",
    "dict_paper_to_journal_id.to_feather('intermediate/dict_paper_to_journal_id.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5594fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
