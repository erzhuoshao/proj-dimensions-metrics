{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73535989",
   "metadata": {},
   "source": [
    "### **Rao-Stirling Diversity Index**\n",
    "\n",
    "The Rao-Stirling diversity index measures the interdisciplinarity of a paper based on the variety and balance of fields represented in its references, weighted by the cognitive distance between those fields.\n",
    "\n",
    "For each paper, we:\n",
    "1. Identify the fields of all cited references\n",
    "2. Calculate the proportion of references in each field (variety and balance)\n",
    "3. Compute pairwise distances between fields based on their cognitive dissimilarity\n",
    "4. Calculate the Rao-Stirling index as the sum of (proportion_i × proportion_j × distance_ij) across all field pairs\n",
    "\n",
    "This approach captures both the diversity of fields cited and how cognitively distant those fields are from each other. Papers citing references from many distant fields will have higher Rao-Stirling scores.\n",
    "\n",
    "**Reference:**\n",
    " - Park, Minsu, et al. \"Interdisciplinary Papers Supported by Disciplinary Grants Garner Deep and Broad Scientific Impact.\" arXiv preprint arXiv:2303.14732 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a58b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import json, pickle as pkl\n",
    "from unidecode import unidecode\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product, combinations\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm, trange\n",
    "tqdm.pandas(ncols=100, mininterval=1)\n",
    "tqdm, trange = partial(tqdm, ncols=100, mininterval=1), partial(trange, ncols=100, mininterval=1)\n",
    "\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faceb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_l0_list = list(pd.read_csv('intermediate/ANZSRC_FoR.tsv', sep='\\t')['Field L0 Code'].drop_duplicates())\n",
    "field_l0_dict = {field_l0_list[i]:i for i in range(len(field_l0_list))}\n",
    "\n",
    "field_l1_list = list(pd.read_csv('intermediate/ANZSRC_FoR.tsv', sep='\\t')['Field L1 Code'].drop_duplicates())\n",
    "field_l1_dict = {field_l1_list[i]:i for i in range(len(field_l1_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ce035",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "papers = pd.read_parquet('../parquet/processed/publications.parquet')\n",
    "paper_field_l0 = pd.read_parquet('../parquet/processed/paper_fields_l0.parquet')\n",
    "paper_field_l1 = pd.read_parquet('../parquet/processed/paper_fields_l1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_to_cited_df = pd.read_feather('intermediate/dict_citing_to_cited_id_and_year.feather')\n",
    "# dict_citing_to_cited = dict_citing_to_cited.set_index(\"citing_paperid\").cited_list.progress_map(lambda x: [i[0] for i in x]).to_dict()\n",
    "# print(f'len(dict_citing_to_cited) = {len(dict_citing_to_cited)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c643dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers2 = papers[['id_INT', 'date_normal', 'field_l0', 'field_l1']]\n",
    "papers2['year'] = papers2['date_normal'].progress_map(lambda x:int(x[:4]))\n",
    "papers2['field_l0'] = papers2['field_l0'].progress_map(lambda x:x[0]).map(field_l0_list)\n",
    "papers2['field_l1'] = papers2['field_l1'].progress_map(lambda x:x[0]).map(field_l1_list)\n",
    "\n",
    "Papers_dict = dict(papers2.apply(lambda x:(x['id_INT'], (x['year'], x['field_l0'], x['field_l1'])), axis=1).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fa1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['field_l0'] = papers.field_l0.map(lambda x:[int(i) for i in x.split('|')])\n",
    "papers['field_l1'] = papers.field_l1.map(lambda x:[int(i) for i in x.split('|')])\n",
    "\n",
    "papers = papers[~papers.num_references.isna()]\n",
    "papers = papers[~papers.date_normal.isna()]\n",
    "papers = papers[papers['num_references'].astype(int) >= 5] # We only keep papers with more than 5 references\n",
    "print(papers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "FieldL1CiteMat = np.zeros([2030, len(L1_dict), len(L1_dict)])\n",
    "for x, y in tqdm(CitingCited[['id_INT', 'reference_INT']].to_numpy()):\n",
    "    x_t, _, x_fs = Papers_dict.get(x, [None, None, None])\n",
    "    y_t, _, y_fs = Papers_dict.get(y, [None, None, None])\n",
    "    if (x_t != None) and (y_t != None):\n",
    "        FieldL1CiteMat[x_t, x_fs, y_fs] += 1\n",
    "                \n",
    "np.save('../tsv/FieldL1CiteMat.npy', FieldL1CiteMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1269e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldL1CiteDistance = FieldL1CiteMat * 0\n",
    "for year in trange(1800, 2030):\n",
    "    for x in range(FieldL1CiteMat.shape[1]):\n",
    "        for y in range(FieldL1CiteMat.shape[2]):\n",
    "            FieldL1CiteDistance[year, x, y] = scipy.spatial.distance.cosine(FieldL1CiteMat[year, x, :], FieldL1CiteMat[year, :, y])\n",
    "            \n",
    "            \n",
    "for x in range(FieldL1CiteDistance.shape[1]):\n",
    "    FieldL1CiteDistance[:, x, x] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../tsv/FieldL1Distance.npy', FieldL1CiteDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aba44a-0b3b-4cf2-858b-6da84fb1320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldL1CiteDistance = np.load('../tsv/FieldL1Distance.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e8d10-fb00-4763-b497-a9aae37d816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CitingCited = pd.read_csv('../tsv/PaperReferences.tsv', sep='\\t')\n",
    "CitingCited['id_INT'] = CitingCited.id.str.slice(4).astype(int)\n",
    "CitingCited['reference_INT'] = CitingCited.reference_ids.str.slice(4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citing2CitedList = CitingCited.groupby('id_INT').reference_INT.apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b354bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df = []\n",
    "for id, v in tqdm(Citing2CitedList.to_numpy()):\n",
    "    year, field_l0, field_1 = zip(*[Papers_dict.get(i, [None, None, None]) for i in v])\n",
    "    \n",
    "    citation_df.append([id, year, field_l0, field_1])\n",
    "    \n",
    "citation_df = pd.DataFrame(citation_df, columns=['id_INT', 'refYear', 'refFieldL0', 'refFieldL1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df.to_parquet('/kellogg/proj/dashun/dimensions/data_dump/20230910/tsv/Intermediate/paper2RefYearFields.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df = pd.read_parquet('/kellogg/proj/dashun/dimensions/data_dump/20230910/tsv/Intermediate/paper2RefYearFields.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cdd45",
   "metadata": {},
   "source": [
    "# Rao-Sterling Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72643f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df['year'] = citation_df.id_INT.progress_map(lambda x:Papers_dict.get(x, [None, None, None])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df = citation_df[~citation_df.year.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f45765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper2RSIndex(refFieldL1, year):\n",
    "    refFieldL1 = refFieldL1\n",
    "    year = year\n",
    "    \n",
    "    field_l1_dist = np.bincount([i for i in refFieldL1 if i >= 0], minlength=213).astype(float)\n",
    "    field_l1_dist /= field_l1_dist.sum()\n",
    "\n",
    "    return 2 * np.sum(field_l1_dist @ FieldL1CiteDistance[int(year)] @ field_l1_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df['RSIndex'] = citation_df.progress_apply(lambda x:paper2RSIndex(x['refFieldL1'], x['year']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df.to_parquet('/kellogg/proj/dashun/dimensions/data_dump/20230910/tsv/Metrics/RaoStirlingDiversity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbea79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper2RSIndex(citation_df['refFieldL1'][0], citation_df['year'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_zipped = spark.read.format(\"json\").option(\"compression\", \"gzip\").option(\"header\", True).load(\n",
    "    \"/kellogg/proj/dashun/dimensions/data_dump/20230121/publications/*\")\n",
    "\n",
    "UsedAuthors = {i:True for i in np.load('./UsedAuthors.npy')}\n",
    "\n",
    "df_zipped2 = df_zipped.select('authors.researcher_id', 'date_normal', 'id', 'category_for.first_level.codes').dropna().rdd.map(lambda x:x.asDict()).map(lambda x:x | {'researcher_id':list(filter(lambda id:UsedAuthors.get(id, False), x['researcher_id']))}).filter(lambda x:len(x['researcher_id']) > 0)\n",
    "print(df_zipped2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68269e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_zipped3 = df_zipped2.flatMap(lambda x:[x|{'researcher_id':id} for id in x['researcher_id']]).groupBy(lambda x:x['researcher_id'])\n",
    "df_zipped4 = df_zipped3.map(lambda x:[x[0], pd.DataFrame(list(x[1])).sort_values('date_normal').reset_index().drop(['researcher_id', 'index'], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zipped4.saveAsPickleFile('/kellogg/proj/dashun/shaoerzhuo/Dimensions/Author2Pubs_SparkPickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10893503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zipped5 = df_zipped4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61162e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('/kellogg/proj/dashun/shaoerzhuo/Dimensions/Author2Pubs.pkl', 'wb') as f:\n",
    "    pkl.dump(df_zipped5, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
